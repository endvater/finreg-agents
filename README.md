# FinRegAgents v2 ğŸ¦ğŸ¤–

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.11%2B-3776ab?logo=python&logoColor=white)](https://python.org)
[![Claude](https://img.shields.io/badge/Powered_by-Claude-d97706)](https://anthropic.com)
[![Status](https://img.shields.io/badge/Status-Alpha-orange)](#)

> **AI Agent Framework fÃ¼r regulatorische PrÃ¼fungen** â€“ GwG, MaRisk, DORA, WpHG/MaComp.

FinRegAgents simuliert behÃ¶rdliche SonderprÃ¼fungen durch spezialisierte KI-Agenten.
Jeder Agent arbeitet einen regulatorischen PrÃ¼fkatalog gegen deine Dokumente ab und
generiert einen formellen PrÃ¼fbericht â€“ so wie es ein BaFin- oder AMLA-PrÃ¼fer tut.

---

## Was ist neu in v2?

Version 2 ist eine vollstÃ¤ndige Ãœberarbeitung basierend auf einem Code-Review, das fÃ¼nf kritische Architektur-SchwÃ¤chen adressiert:

| Problem in v1 | LÃ¶sung in v2 |
|---|---|
| Keine Verifikationsschicht â€“ Halluzinationen landen ungeprÃ¼ft im Bericht | **Retrieval-Quality-Gate** + **Strukturelle Validierung** + **Confidence-Scoring** |
| System-Prompt ist GwG-hardcoded â€“ DORA wird von einem "GwG-PrÃ¼fer" bewertet | **Regulatorik-spezifische System-Prompts** fÃ¼r jede der 4 Regulatoriken |
| `nicht_prÃ¼fbar` wird ignoriert â€“ 80% nicht prÃ¼fbar = "KONFORM" | **Evidenz-Warnungen**: Ab 30% nicht prÃ¼fbar wird die Gesamtbewertung eingeschrÃ¤nkt |
| XSS im HTML-Report â€“ Befund-Texte ungefiltert eingebettet | **html.escape()** fÃ¼r alle dynamischen Inhalte |
| Kein Audit-Trail, kein Checkpoint | **Audit-Trail** (Modell, Katalog-Version) + **Checkpoint** nach jeder Sektion |

### Weitere Verbesserungen

- **Confidence-Score** (0.0â€“1.0) pro Befund aus vier Signalen: Retrieval-Score, Evidenz-Coverage, Type-Match, LLM-Self-Assessment
- **Review-Markierung**: Befunde unter dem Confidence-Threshold werden als "Review erforderlich" markiert
- **Sektions-Eskalation**: Wenn >30% der Befunde einer Sektion Review erfordern â†’ Warnung
- **Interview-Parsing**: UnterstÃ¼tzt jetzt beide JSON-Formate (Array und Dict mit `fragen_antworten`)
- **Screenshot-Memory-Fix**: base64-Daten werden nicht mehr in den Index geladen
- **Chunk-Size**: Von 512 auf 1024 Tokens erhÃ¶ht (besser fÃ¼r regulatorische Texte)
- **Deduplizierung**: Identische Dateien in verschiedenen Ordnern werden nur einmal indexiert
- **YAML-Support**: Interview-FragebÃ¶gen in YAML werden korrekt geparst
- **Model-Default**: Sonnet statt Opus (kosteneffizient, Opus optional per `--model`)
- **Test-Suite**: Pytest-Tests fÃ¼r Confidence, Validierung, JSON-Parsing, Katalog-Struktur
- **Keine globale LlamaIndex-State-Mutation** mehr

---

## Architektur

```
finreg-agents/
â”‚
â”œâ”€â”€ pipeline.py              â† Hauptorchestrator (CLI + Python API)
â”‚
â”œâ”€â”€ catalog/
â”‚   â”œâ”€â”€ gwg_catalog.json     â† GwG-PrÃ¼fkatalog (34 PrÃ¼ffelder, 8 Sektionen)
â”‚   â”œâ”€â”€ dora_catalog.json    â† DORA-Katalog (18 PrÃ¼ffelder, 5 Sektionen)
â”‚   â”œâ”€â”€ marisk_catalog.json  â† MaRisk-Katalog (22 PrÃ¼ffelder, 8 Sektionen)
â”‚   â””â”€â”€ wphg_catalog.json    â† WpHG/MaComp-Katalog (20 PrÃ¼ffelder, 7 Sektionen)
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ ingestor.py          â† Multi-Modal Document Ingestor
â”‚   â””â”€â”€ interviews/          â† Beispiel-FragebÃ¶gen
â”‚
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ pruef_agent.py       â† RAG + LLM PrÃ¼fer-Agent + Validierung + Confidence
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ bericht_generator.py â† PrÃ¼fbericht (JSON / MD / HTML) mit Audit-Trail
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_core.py         â† Pytest-Tests fÃ¼r Kernkomponenten
```

### Datenfluss

```
Dokumente (PDF, Excel, Interview, Screenshot, Log)
        â”‚
        â–¼
  [GwGIngestor]              Multi-Modal Ingestion, Chunking, Dedup
        â”‚
        â–¼
  [VectorStoreIndex]         LlamaIndex + OpenAI Embeddings
        â”‚
        â–¼
  [PrÃ¼fkatalog]              94 PrÃ¼ffelder in 4 Regulatoriken
        â”‚
        â”‚   fÃ¼r jedes PrÃ¼ffeld:
        â–¼
  [PrueferAgent]
   â”œâ”€ RAG-Retrieval          â†’ Top-k relevante Chunks holen
   â”œâ”€ Quality-Gate       NEU â†’ Score < Threshold? â†’ nicht_prÃ¼fbar (kein LLM-Call)
   â”œâ”€ LLM-Bewertung          â†’ Regulatorik-spezifischer Prompt â†’ Claude
   â”œâ”€ Strukturelle Valid. NEU â†’ Quellen-Cross-Check, Platzhalter, Konsistenz
   â””â”€ Confidence-Score   NEU â†’ 4 Signale â†’ Score + Review-Markierung
        â”‚
        â–¼
  [Checkpoint]           NEU â†’ Zwischenergebnis nach jeder Sektion
        â”‚
        â–¼
  [BerichtGenerator]
   â”œâ”€ JSON + Markdown + HTML
   â”œâ”€ Confidence-Bars    NEU â†’ Visuelle Confidence-Indikatoren
   â”œâ”€ Evidenz-Warnungen  NEU â†’ Warnung bei hohem nicht_prÃ¼fbar-Anteil
   â””â”€ Audit-Trail        NEU â†’ Modell, Katalog-Version, Zeitstempel
```

---

## UnterstÃ¼tzte Regulatorik

| Regulatorik | Sektionen | PrÃ¼ffelder | Rechtsgrundlage |
|---|---|---|---|
| **GwG / AML** | 8 | 34 | GwG, Â§25h KWG, BaFin AuA |
| **DORA** | 5 | 18 | DORA Art. 5-46, RTS |
| **MaRisk** | 8 | 22 | MaRisk AT/BT, Â§25a KWG |
| **WpHG / MaComp** | 7 | 20 | WpHG, MaComp, MAR, MiFID II |

---

## Quickstart

### 1. Installation

```bash
git clone https://github.com/endvater/finreg-agents.git
cd finreg-agents
pip install -r requirements.txt
```

### 2. API-Keys setzen

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
export OPENAI_API_KEY="sk-..."        # fÃ¼r Embeddings (text-embedding-3-small)
```

### 3. Dokumente ablegen

```
meine_dokumente/
  pdfs/           â†’ Policies, Verfahrensanweisungen, PrÃ¼fberichte (*.pdf)
  excel/          â†’ Alert-Statistiken, Schulungsnachweise (*.xlsx, *.csv)
  interviews/     â†’ BefragungsbÃ¶gen (*.json, *.yaml)
  screenshots/    â†’ TM-System, goAML, KYC-OberflÃ¤che (*.png, *.jpg)
  logs/           â†’ Systemlogs, Auditlogs (*.txt, *.log)
```

### 4. PrÃ¼fung starten

```bash
# GwG-SonderprÃ¼fung (AML) â€“ Default: Sonnet (kosteneffizient)
python pipeline.py --input ./docs --institution "Musterbank AG" --regulatorik gwg

# DORA-PrÃ¼fung (nur Drittparteienrisiko)
python pipeline.py --input ./docs --regulatorik dora --sektionen D04

# MaRisk-VollprÃ¼fung mit Opus (hÃ¶chste QualitÃ¤t)
python pipeline.py --input ./docs --regulatorik marisk --model claude-opus-4-5

# WpHG / MaComp
python pipeline.py --input ./docs --regulatorik wphg --sektionen W02 W03 W04
```

### 5. Python API

```python
from pipeline import AuditPipeline

pipeline = AuditPipeline(
    input_dir="./meine_dokumente",
    institution="Musterbank AG",
    regulatorik="dora",
    sektionen_filter=["D01", "D02"],   # optional: TeilprÃ¼fung
    model="claude-sonnet-4-5-20250514", # optional: Modellwahl
)
report_paths = pipeline.run()
# â†’ {"json": "...", "markdown": "...", "html": "..."}
```

### 6. Tests ausfÃ¼hren

```bash
pytest tests/ -v
```

---

## Confidence-Scoring

Jeder Befund erhÃ¤lt einen Confidence-Score (0.0â€“1.0), der aus vier Signalen berechnet wird:

| Signal | Gewichtung | Beschreibung |
|---|---|---|
| Retrieval-Score | 30% | Durchschnittliche Relevanz der gefundenen Chunks |
| Evidenz-Coverage | 30% | Anteil der erwarteten Evidenz, die gefunden wurde |
| Type-Match | 20% | Stimmen die Dokumenttypen (PDF, Excel, etc.) Ã¼berein? |
| LLM-Self-Assessment | 20% | SelbsteinschÃ¤tzung des Modells |

### Schwellenwerte

| Confidence | Aktion |
|---|---|
| < 0.40 | Automatisch `nicht_prÃ¼fbar` â€“ LLM-Bewertung wird Ã¼berschrieben |
| 0.40 â€“ 0.70 | Befund markiert als **Review erforderlich** ğŸ” |
| > 0.70 | Befund geht in den Bericht |
| >30% Review in einer Sektion | **Sektions-Eskalation** empfohlen |

---

## Strukturelle Validierung

Vor der Aufnahme in den Bericht durchlÃ¤uft jeder Befund automatische Checks:

- **Quellen-Cross-Check**: Zitiert der Agent Quellen, die nicht im Retrieval waren? â†’ Phantom-Quellen-Warnung
- **Platzhalter-Check**: UnaufgelÃ¶ste `{}`-Platzhalter in BegrÃ¼ndungen oder Mangel-Texten
- **Konsistenz-Check**: `konform` ohne Textstellen? `nicht_konform` ohne Mangel-Text?
- **Bewertungs-Konsistenz**: Mangel-Text bei `konform`-Bewertung?

Alle Warnungen werden im Befund gespeichert und im Bericht angezeigt.

---

## Bewertungsskala

| Bewertung | Bedeutung |
|---|---|
| âœ… **konform** | Anforderung vollstÃ¤ndig erfÃ¼llt, Evidenz vorhanden |
| âš ï¸ **teilkonform** | Anforderung teilweise erfÃ¼llt, Nachbesserung erforderlich |
| ğŸ”´ **nicht_konform** | Anforderung nicht erfÃ¼llt â€“ Mangel im Bericht |
| â“ **nicht_prÃ¼fbar** | Keine ausreichende Evidenz im PrÃ¼fungskorpus |

**Schweregrade:** `wesentlich` (sofortiger Handlungsbedarf) Â· `bedeutsam` Â· `gering`

### Gesamtbewertungslogik (v2)

| Bedingung | Gesamtbewertung |
|---|---|
| Wesentliche MÃ¤ngel vorhanden | **ERHEBLICHE MÃ„NGEL** |
| â‰¥50% nicht prÃ¼fbar | **UNZUREICHENDE EVIDENZ â€“ PRÃœFUNG NICHT BELASTBAR** |
| MÃ¤ngel oder â‰¥3 teilkonform | **MÃ„NGEL FESTGESTELLT** |
| â‰¥30% nicht prÃ¼fbar | **EINGESCHRÃ„NKT BELASTBAR** |
| Teilkonforme Befunde vorhanden | **TEILKONFORM â€“ NACHBESSERUNG ERFORDERLICH** |
| Alles konform | **KONFORM** |

---

## Eigenen Katalog erstellen

Jedes PrÃ¼ffeld folgt diesem Schema:

```json
{
  "katalog_version": "2025-01",
  "basis": ["MaRisk 2023", "BaFin-Rundschreiben"],
  "pruefsektionen": [
    {
      "id": "S01",
      "titel": "Interne Kontrollsysteme",
      "rechtsgrundlagen": ["MaRisk AT 4.3"],
      "prueffelder": [
        {
          "id": "S01-01",
          "frage": "Ist ein IKS dokumentiert und implementiert?",
          "erwartete_evidenz": ["IKS-Dokumentation", "Prozesshandbuch"],
          "input_typen": ["pdf", "interview"],
          "bewertungskriterien": "Schriftliche IKS-Dokumentation muss vorliegen",
          "schweregrad": "wesentlich",
          "mangel_template": "Ein dokumentiertes IKS gemÃ¤ÃŸ MaRisk AT 4.3 fehlt."
        }
      ]
    }
  ]
}
```

```bash
python pipeline.py --input ./docs --catalog ./mein_katalog.json
```

---

## Interview-Format

Strukturierte Befragungsprotokolle werden direkt in den Index aufgenommen.
UnterstÃ¼tzt werden zwei JSON-Formate:

**Format A â€“ Dict mit Metadaten (empfohlen):**

```json
{
  "meta": {
    "institut": "Musterbank AG",
    "datum": "2025-02-01",
    "interviewer": "PrÃ¼fer KI"
  },
  "fragen_antworten": [
    {
      "id": "I-01",
      "prueffeld_referenz": "S04-01",
      "frage": "Seit wann sind Sie als GwB bestellt?",
      "antwort": "Seit MÃ¤rz 2022, BaFin-Meldung am 20.03.2022.",
      "kommentar": "Bestellungsbeschluss liegt vor."
    }
  ]
}
```

**Format B â€“ Einfaches Array:**

```json
[
  {"frage": "...", "antwort": "...", "kommentar": "..."}
]
```

---

## PrÃ¼fbericht-Output

Jede PrÃ¼fung erzeugt drei Dateien:

| Format | Verwendung |
|---|---|
| **JSON** | Maschinenlesbar, API-Integration, Weiterverarbeitung |
| **Markdown** | Lesbar, Git-kompatibel, Review-Workflows |
| **HTML** | DruckfÃ¤hig, PrÃ¤sentation, PDF-Konvertierung |

Alle Berichte enthalten jetzt:
- Confidence-Bars pro Befund
- Review-Markierungen (ğŸ”) fÃ¼r unsichere Bewertungen
- Validierungshinweise (âš¡) bei strukturellen Problemen
- Evidenz-Warnungen bei hohem nicht_prÃ¼fbar-Anteil
- Audit-Trail mit Modell, Katalog-Version und Zeitstempel

---

## Kosten-EinschÃ¤tzung

| Regulatorik | PrÃ¼ffelder | GeschÃ¤tzter Aufwand (Sonnet) | GeschÃ¤tzter Aufwand (Opus) |
|---|---|---|---|
| GwG | 34 | ~$0.80â€“1.50 | ~$8â€“15 |
| DORA | 18 | ~$0.40â€“0.80 | ~$4â€“8 |
| MaRisk | 22 | ~$0.50â€“1.00 | ~$5â€“10 |
| WpHG | 20 | ~$0.45â€“0.90 | ~$4.50â€“9 |

Hinweis: Kosten hÃ¤ngen von Dokumentenmenge, Chunk-Anzahl und AntwortlÃ¤nge ab. Durch das Retrieval-Quality-Gate in v2 werden unnÃ¶tige LLM-Calls bei schlechtem Retrieval eingespart.

---

## Roadmap

- [ ] Skeptiker-Agent: Adversariales LLM-Review als optionaler Post-Processing-Layer
- [ ] Synthetische Kontroll-PrÃ¼ffelder (Ground-Truth-Signal) zur Kalibrierung
- [ ] Persistenter Vektorindex via ChromaDB / Weaviate
- [ ] Claude Vision fÃ¼r Screenshot-Analyse (TM-Systeme, KYC-OberflÃ¤chen)
- [ ] Delta-PrÃ¼fung â€“ nur geÃ¤nderte Dokumente neu einlesen
- [ ] Streamlit-UI fÃ¼r interaktive PrÃ¼fung mit Sampling-Audit
- [ ] JSON-Schema fÃ¼r Custom-Kataloge mit Validierung beim Laden
- [ ] Multi-Institut-Vergleich â€“ Benchmarking Ã¼ber Institutsgrenzen

---

## Disclaimer

FinRegAgents ist ein **Simulations- und Vorbereitungstool**. Es ersetzt **keine
offizielle BaFin-PrÃ¼fung** und begrÃ¼ndet keine Rechtsberatung. PrÃ¼fungsergebnisse
sind als interne Vorbereitung zu verstehen, nicht als behÃ¶rdliche Feststellung.

Die Confidence-Scores und Review-Markierungen dienen dazu, die Belastbarkeit
der einzelnen Befunde transparent zu machen. Befunde mit niedrigem Confidence
oder Review-Markierung sollten stets manuell validiert werden.

---

## Contributing

Contributions willkommen â€“ insbesondere:

- Neue PrÃ¼fkataloge fÃ¼r weitere Regulatoriken
- Verbesserte PrÃ¼ffragen und Bewertungskriterien
- Skeptiker-Agent-Implementierung
- Neue Ingestion-Adapter (z.B. .docx, Notion, Confluence)
- Tests und Benchmarks

Bitte fork â†’ branch â†’ PR mit Beschreibung welche Regulatorik / welches Feature erweitert wurde.

---

## Lizenz

Apache License 2.0 â€“ siehe [LICENSE](LICENSE).

Du kannst FinRegAgents frei nutzen, modifizieren und in kommerzielle Produkte
integrieren, solange der Copyright-Vermerk erhalten bleibt.

---

Gebaut mit LlamaIndex Â· LangChain Â· Claude Â· â¤ï¸
